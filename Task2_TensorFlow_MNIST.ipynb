# mnist_cnn_tf.py
# Requirements: tensorflow (>=2.6), matplotlib, numpy
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt

# Load data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# reshape for conv: (samples, 28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

# Build model
def build_model():
    model = models.Sequential([
        layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3,3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2,2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.4),
        layers.Dense(10, activation='softmax')
    ])
    return model

model = build_model()
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train
callbacks = [
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2),
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)
]
history = model.fit(x_train, y_train, epochs=15, batch_size=128, validation_split=0.1, callbacks=callbacks)

# Evaluate
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print("Test accuracy:", test_acc)

# Visualize predictions on 5 sample images
import random
idxs = random.sample(range(len(x_test)), 5)
preds = model.predict(x_test[idxs])
pred_labels = preds.argmax(axis=1)

plt.figure(figsize=(12,3))
for i, idx in enumerate(idxs):
    plt.subplot(1,5,i+1)
    plt.imshow(x_test[idx].squeeze(), cmap='gray')
    plt.title(f"pred:{pred_labels[i]} / true:{y_test[idx]}")
    plt.axis('off')
plt.show()

